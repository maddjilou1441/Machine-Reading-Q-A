{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AtCN5wIDeqC"
   },
   "source": [
    "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "id": "IcTy9yd5cHLi",
    "outputId": "86dc3526-f0f3-4a09-ea9e-755ce2e1d1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/root/ParlAI'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 31659 (delta 37), reused 36 (delta 18), pack-reused 31578\u001b[K\n",
      "Receiving objects: 100% (31659/31659), 58.95 MiB | 3.73 MiB/s, done.\n",
      "Resolving deltas: 100% (22587/22587), done.\n",
      "Note: checking out '6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 6bd0e586 light paper link on webpage! (#1536)\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "  File \"build/bdist.linux-x86_64/egg/websocket/policyserver.py\", line 21\n",
      "    print 'Accepted connection from %s:%s' % address\n",
      "                                         ^\n",
      "SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Accepted connection from %s:%s' % address)?\n",
      "\n",
      "  File \"build/bdist.linux-x86_64/egg/websocket/server.py\", line 57\n",
      "    except IOError, ex:\n",
      "                  ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/websocket-0.2.1-py3.6.egg/websocket/policyserver.py\", line 21\n",
      "    print 'Accepted connection from %s:%s' % address\n",
      "                                         ^\n",
      "SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Accepted connection from %s:%s' % address)?\n",
      "\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/websocket-0.2.1-py3.6.egg/websocket/server.py\", line 57\n",
      "    except IOError, ex:\n",
      "                  ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove `> /dev/null` to see the output of commands\n",
    "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
    "!cd ~/ParlAI && git checkout 6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3\n",
    "!cd ~/ParlAI && python3 setup.py develop > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrD0zFAvCoWI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrk86MpLCqjI"
   },
   "source": [
    "# Here we are looking for the location where our functions we need to use are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "3X0YYwEAcelW",
    "outputId": "1c5f37b7-3b83-42eb-b432-91deb735e8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_train.py\t       eval_model.py\t\t remote.py\n",
      "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
      "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
      "display_data.py        profile_train.py\n",
      "display_model.py       README.md\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/examples/interactive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "Wqx-QlR7-eY9",
    "outputId": "874718f7-cbfc-4742-fd43-419f2415ed77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_dict.py\t\t\t    eval_ppl.py\n",
      "build_pytorch_data.py\t\t    eval_wordstat.py\n",
      "convert_data_to_fasttext_format.py  extract_image_feature.py\n",
      "convert_data_to_parlai_format.py    __init__.py\n",
      "data_stats.py\t\t\t    interactive.py\n",
      "detect_offensive_language.py\t    interactive_rank.py\n",
      "display_data.py\t\t\t    multiprocessing_train.py\n",
      "display_model.py\t\t    profile_train.py\n",
      "distributed_train.py\t\t    train_model.py\n",
      "eval_model.py\t\t\t    verify_data.py\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/parlai/scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNA1v8mWCNXi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8knh0DgCXS7"
   },
   "source": [
    "# Displaying the data on the babi task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tPIo9ZXSXSHX",
    "outputId": "5c43366b-becf-44c8-c276-86bfbf2880fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields: agent_reply ]\n",
      "[  max_display_len: 1000 ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train:stream ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[building data: /root/ParlAI/data/bAbI]\n",
      "[ downloading: http://parl.ai/downloads/babi/babi.tar.gz to /root/ParlAI/data/bAbI/babi.tar.gz ]\n",
      "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:03<00:00, 5.01MB/s]\n",
      "unpacking babi.tar.gz\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[babi:task10k:1]: Mary moved to the bathroom.\n",
      "John went to the hallway.\n",
      "Where is Mary?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bathroom]\n",
      "~~\n",
      "[babi:task10k:1]: Daniel went back to the hallway.\n",
      "Sandra moved to the garden.\n",
      "Where is Daniel?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: hallway]\n",
      "~~\n",
      "[babi:task10k:1]: John moved to the office.\n",
      "Sandra journeyed to the bathroom.\n",
      "Where is Daniel?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: hallway]\n",
      "~~\n",
      "[babi:task10k:1]: Mary moved to the hallway.\n",
      "Daniel travelled to the office.\n",
      "Where is Daniel?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: office]\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the garden.\n",
      "John moved to the bedroom.\n",
      "Where is Sandra?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bathroom]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "John journeyed to the bathroom.\n",
      "Where is John?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bathroom]\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "John journeyed to the garden.\n",
      "Where is Mary?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bedroom]\n",
      "~~\n",
      "[babi:task10k:1]: John journeyed to the bathroom.\n",
      "Sandra journeyed to the garden.\n",
      "Where is John?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bathroom]\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bedroom.\n",
      "Daniel travelled to the bathroom.\n",
      "Where is John?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bathroom]\n",
      "~~\n",
      "[babi:task10k:1]: John went to the office.\n",
      "Mary moved to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: kitchen|garden|bedroom|hallway|bathroom|...and 1 more]\n",
      "[labels: bedroom]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[ loaded 1800 episodes with a total of 9000 examples ]\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_data.py -t babi:task10k:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCgfFJZNBw8E"
   },
   "source": [
    "# Training the model memnn on the task babi:task10k:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "C0zgRUhaXmLW",
    "outputId": "8cce372a-9a78-410d-b0e3-12325b6a2808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 4 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: memnn ]\n",
      "[  model_file: /tmp/babi_memnn ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  dict_build_first: True ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  num_epochs: 5.0 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ MemNN Arguments: ] \n",
      "[  embedding_size: 128 ]\n",
      "[  hops: 3 ]\n",
      "[  memsize: 32 ]\n",
      "[  position_encoding: False ]\n",
      "[  time_features: True ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: True ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: True ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: True ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ TorchRankerAgent: ] \n",
      "[  candidates: inline ]\n",
      "[  cap_num_predictions: 100 ]\n",
      "[  encode_candidate_vecs: False ]\n",
      "[  eval_candidates: inline ]\n",
      "[  fixed_candidate_vecs: reuse ]\n",
      "[  fixed_candidates_path: None ]\n",
      "[  ignore_bad_candidates: False ]\n",
      "[  train_predict: False ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ building dictionary first... ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[ running dictionary over data.. ]\n",
      "Building dictionary:   0% 0.00/9.00k [00:00<?, ?ex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary:  78% 7.00k/9.00k [00:00<00:00, 23.3kex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 22.9kex/s]\n",
      "Dictionary: saving dictionary to /tmp/babi_memnn.dict\n",
      "[ dictionary built with 26 tokens in 0s ]\n",
      "[ no model with opt yet at: /tmp/babi_memnn(.opt) ]\n",
      "Dictionary: loading dictionary from /tmp/babi_memnn.dict\n",
      "[ num words =  26 ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[ thread 0 initialized ]\n",
      "[ thread 1 initialized ]\n",
      "[ thread 2 initialized ]\n",
      "[ thread 3 initialized ]\n",
      "[ training... ]\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
      "  \"Some training metrics are omitted for speed. Set the flag \"\n",
      "[ time:2.0s total_exs:597 epochs:0.07 time_left:150.0s ] {'exs': 597, 'lr': 1, 'num_updates': 0, 'examples': 588, 'loss': 2705.0, 'mean_loss': 4.6, 'mean_rank': 3.053}\n",
      "[ time:4.0s total_exs:1275 epochs:0.14 time_left:138.0s ] {'exs': 678, 'lr': 1, 'num_updates': 0, 'examples': 665, 'loss': 1210.0, 'mean_loss': 1.82, 'mean_rank': 1.785}\n",
      "[ time:6.0s total_exs:1945 epochs:0.22 time_left:134.0s ] {'exs': 670, 'lr': 1, 'num_updates': 0, 'examples': 655, 'loss': 482.6, 'mean_loss': 0.7368, 'mean_rank': 1.206}\n",
      "[ time:8.0s total_exs:2617 epochs:0.29 time_left:130.0s ] {'exs': 672, 'lr': 1, 'num_updates': 0, 'examples': 657, 'loss': 331.4, 'mean_loss': 0.5045, 'mean_rank': 1.14}\n",
      "[ time:10.0s total_exs:3289 epochs:0.37 time_left:128.0s ] {'exs': 672, 'lr': 1, 'num_updates': 0, 'examples': 662, 'loss': 215.1, 'mean_loss': 0.325, 'mean_rank': 1.088}\n",
      "[ time:12.0s total_exs:3970 epochs:0.44 time_left:125.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 657, 'loss': 246.9, 'mean_loss': 0.3758, 'mean_rank': 1.078}\n",
      "[ time:14.0s total_exs:4647 epochs:0.52 time_left:122.0s ] {'exs': 677, 'lr': 1, 'num_updates': 0, 'examples': 659, 'loss': 155.8, 'mean_loss': 0.2364, 'mean_rank': 1.049}\n",
      "[ time:16.0s total_exs:5328 epochs:0.59 time_left:120.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 655, 'loss': 166.6, 'mean_loss': 0.2544, 'mean_rank': 1.058}\n",
      "[ time:18.0s total_exs:6008 epochs:0.67 time_left:117.0s ] {'exs': 680, 'lr': 1, 'num_updates': 0, 'examples': 658, 'loss': 112.9, 'mean_loss': 0.1716, 'mean_rank': 1.056}\n",
      "[ time:20.0s total_exs:6683 epochs:0.74 time_left:115.0s ] {'exs': 675, 'lr': 1, 'num_updates': 0, 'examples': 660, 'loss': 54.79, 'mean_loss': 0.08301, 'mean_rank': 1.015}\n",
      "[ time:22.0s total_exs:7352 epochs:0.82 time_left:113.0s ] {'exs': 669, 'lr': 1, 'num_updates': 0, 'examples': 659, 'loss': 82.26, 'mean_loss': 0.1248, 'mean_rank': 1.021}\n",
      "[ time:24.0s total_exs:8033 epochs:0.89 time_left:111.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 658, 'loss': 129.9, 'mean_loss': 0.1975, 'mean_rank': 1.03}\n",
      "[ time:26.0s total_exs:8711 epochs:0.97 time_left:109.0s ] {'exs': 678, 'lr': 1, 'num_updates': 0, 'examples': 655, 'loss': 65.27, 'mean_loss': 0.09965, 'mean_rank': 1.026}\n",
      "[ time:28.0s total_exs:9397 epochs:1.04 time_left:107.0s ] {'exs': 686, 'lr': 1, 'num_updates': 0, 'examples': 661, 'loss': 42.38, 'mean_loss': 0.06412, 'mean_rank': 1.021}\n",
      "[ time:30.0s total_exs:10082 epochs:1.12 time_left:105.0s ] {'exs': 685, 'lr': 1, 'num_updates': 0, 'examples': 666, 'loss': 61.81, 'mean_loss': 0.09282, 'mean_rank': 1.018}\n",
      "[ time:32.0s total_exs:10758 epochs:1.2 time_left:102.0s ] {'exs': 676, 'lr': 1, 'num_updates': 0, 'examples': 652, 'loss': 44.07, 'mean_loss': 0.0676, 'mean_rank': 1.023}\n",
      "[ time:34.0s total_exs:11442 epochs:1.27 time_left:100.0s ] {'exs': 684, 'lr': 1, 'num_updates': 0, 'examples': 672, 'loss': 60.43, 'mean_loss': 0.08993, 'mean_rank': 1.012}\n",
      "[ time:36.0s total_exs:12107 epochs:1.35 time_left:98.0s ] {'exs': 665, 'lr': 1, 'num_updates': 0, 'examples': 655, 'loss': 34.9, 'mean_loss': 0.05328, 'mean_rank': 1.009}\n",
      "[ time:38.0s total_exs:12788 epochs:1.42 time_left:96.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 663, 'loss': 29.83, 'mean_loss': 0.04499, 'mean_rank': 1.023}\n",
      "[ time:40.0s total_exs:13471 epochs:1.5 time_left:94.0s ] {'exs': 683, 'lr': 1, 'num_updates': 0, 'examples': 665, 'loss': 57.91, 'mean_loss': 0.08708, 'mean_rank': 1.021}\n",
      "[ time:42.0s total_exs:14130 epochs:1.57 time_left:92.0s ] {'exs': 659, 'lr': 1, 'num_updates': 0, 'examples': 640, 'loss': 34.2, 'mean_loss': 0.05344, 'mean_rank': 1.014}\n",
      "[ time:44.0s total_exs:14813 epochs:1.65 time_left:90.0s ] {'exs': 683, 'lr': 1, 'num_updates': 0, 'examples': 670, 'loss': 54.2, 'mean_loss': 0.0809, 'mean_rank': 1.007}\n",
      "[ time:46.0s total_exs:15508 epochs:1.72 time_left:88.0s ] {'exs': 695, 'lr': 1, 'num_updates': 0, 'examples': 677, 'loss': 76.61, 'mean_loss': 0.1132, 'mean_rank': 1.015}\n",
      "[ time:48.0s total_exs:16176 epochs:1.8 time_left:86.0s ] {'exs': 668, 'lr': 1, 'num_updates': 0, 'examples': 655, 'loss': 65.75, 'mean_loss': 0.1004, 'mean_rank': 1.014}\n",
      "[ time:50.0s total_exs:16865 epochs:1.87 time_left:84.0s ] {'exs': 689, 'lr': 1, 'num_updates': 0, 'examples': 674, 'loss': 12.33, 'mean_loss': 0.01829, 'mean_rank': 1.007}\n",
      "[ time:52.0s total_exs:17528 epochs:1.95 time_left:82.0s ] {'exs': 663, 'lr': 1, 'num_updates': 0, 'examples': 648, 'loss': 19.38, 'mean_loss': 0.02991, 'mean_rank': 1.003}\n",
      "[ time:54.0s total_exs:18186 epochs:2.02 time_left:80.0s ] {'exs': 658, 'lr': 1, 'num_updates': 0, 'examples': 647, 'loss': 19.48, 'mean_loss': 0.03011, 'mean_rank': 1.006}\n",
      "[ time:56.0s total_exs:18865 epochs:2.1 time_left:78.0s ] {'exs': 679, 'lr': 1, 'num_updates': 0, 'examples': 658, 'loss': 39.65, 'mean_loss': 0.06025, 'mean_rank': 1.014}\n",
      "[ time:58.0s total_exs:19533 epochs:2.17 time_left:76.0s ] {'exs': 668, 'lr': 1, 'num_updates': 0, 'examples': 661, 'loss': 2.549, 'mean_loss': 0.003856, 'mean_rank': 0.9939}\n",
      "[ time:60.0s total_exs:20214 epochs:2.25 time_left:74.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 660, 'loss': 6.122, 'mean_loss': 0.009275, 'mean_rank': 1.005}\n",
      "[ time:62.0s total_exs:20906 epochs:2.32 time_left:72.0s ] {'exs': 692, 'lr': 1, 'num_updates': 0, 'examples': 677, 'loss': 34.95, 'mean_loss': 0.05162, 'mean_rank': 1.007}\n",
      "[ time:64.0s total_exs:21592 epochs:2.4 time_left:70.0s ] {'exs': 686, 'lr': 1, 'num_updates': 0, 'examples': 671, 'loss': 27.21, 'mean_loss': 0.04056, 'mean_rank': 1.012}\n",
      "[ time:66.0s total_exs:22263 epochs:2.47 time_left:68.0s ] {'exs': 671, 'lr': 1, 'num_updates': 0, 'examples': 653, 'loss': 20.6, 'mean_loss': 0.03154, 'mean_rank': 1.023}\n",
      "[ time:68.0s total_exs:22954 epochs:2.55 time_left:66.0s ] {'exs': 691, 'lr': 1, 'num_updates': 0, 'examples': 677, 'loss': 30.21, 'mean_loss': 0.04463, 'mean_rank': 1.001}\n",
      "[ time:70.0s total_exs:23639 epochs:2.63 time_left:64.0s ] {'exs': 685, 'lr': 1, 'num_updates': 0, 'examples': 670, 'loss': 54.6, 'mean_loss': 0.08149, 'mean_rank': 1.009}\n",
      "[ time:72.0s total_exs:24314 epochs:2.7 time_left:62.0s ] {'exs': 675, 'lr': 1, 'num_updates': 0, 'examples': 656, 'loss': 13.54, 'mean_loss': 0.02064, 'mean_rank': 1.014}\n",
      "[ time:74.0s total_exs:24998 epochs:2.78 time_left:60.0s ] {'exs': 684, 'lr': 1, 'num_updates': 0, 'examples': 659, 'loss': 2.681, 'mean_loss': 0.004068, 'mean_rank': 1.0}\n",
      "[ time:76.0s total_exs:25670 epochs:2.85 time_left:58.0s ] {'exs': 672, 'lr': 1, 'num_updates': 0, 'examples': 652, 'loss': 5.923, 'mean_loss': 0.009085, 'mean_rank': 1.002}\n",
      "[ time:78.0s total_exs:26348 epochs:2.93 time_left:56.0s ] {'exs': 678, 'lr': 1, 'num_updates': 0, 'examples': 667, 'loss': 12.94, 'mean_loss': 0.0194, 'mean_rank': 1.003}\n",
      "[ time:80.0s total_exs:26979 epochs:3.0 time_left:54.0s ] {'exs': 631, 'lr': 1, 'num_updates': 0, 'examples': 616, 'loss': 9.229, 'mean_loss': 0.01498, 'mean_rank': 1.002}\n",
      "[ time:82.0s total_exs:27616 epochs:3.07 time_left:52.0s ] {'exs': 637, 'lr': 1, 'num_updates': 0, 'examples': 623, 'loss': 19.01, 'mean_loss': 0.03051, 'mean_rank': 1.002}\n",
      "[ time:84.0s total_exs:28308 epochs:3.15 time_left:50.0s ] {'exs': 692, 'lr': 1, 'num_updates': 0, 'examples': 676, 'loss': 8.61, 'mean_loss': 0.01274, 'mean_rank': 1.001}\n",
      "[ time:86.0s total_exs:28984 epochs:3.22 time_left:48.0s ] {'exs': 676, 'lr': 1, 'num_updates': 0, 'examples': 652, 'loss': 21.69, 'mean_loss': 0.03326, 'mean_rank': 1.005}\n",
      "[ time:88.0s total_exs:29664 epochs:3.3 time_left:46.0s ] {'exs': 680, 'lr': 1, 'num_updates': 0, 'examples': 657, 'loss': 16.49, 'mean_loss': 0.0251, 'mean_rank': 1.012}\n",
      "[ time:90.0s total_exs:30344 epochs:3.37 time_left:44.0s ] {'exs': 680, 'lr': 1, 'num_updates': 0, 'examples': 659, 'loss': 23.03, 'mean_loss': 0.03495, 'mean_rank': 1.005}\n",
      "[ time:92.0s total_exs:31017 epochs:3.45 time_left:42.0s ] {'exs': 673, 'lr': 1, 'num_updates': 0, 'examples': 650, 'loss': 10.46, 'mean_loss': 0.01609, 'mean_rank': 1.006}\n",
      "[ time:94.0s total_exs:31656 epochs:3.52 time_left:40.0s ] {'exs': 639, 'lr': 1, 'num_updates': 0, 'examples': 617, 'loss': 5.738, 'mean_loss': 0.0093, 'mean_rank': 1.002}\n",
      "[ time:96.0s total_exs:32312 epochs:3.59 time_left:38.0s ] {'exs': 656, 'lr': 1, 'num_updates': 0, 'examples': 634, 'loss': 4.558, 'mean_loss': 0.00719, 'mean_rank': 0.9968}\n",
      "[ time:98.0s total_exs:32973 epochs:3.66 time_left:36.0s ] {'exs': 660, 'lr': 1, 'num_updates': 0, 'examples': 643, 'loss': 8.645, 'mean_loss': 0.01344, 'mean_rank': 1.006}\n",
      "[ time:100.0s total_exs:33616 epochs:3.74 time_left:34.0s ] {'exs': 643, 'lr': 1, 'num_updates': 0, 'examples': 625, 'loss': 34.4, 'mean_loss': 0.05504, 'mean_rank': 1.011}\n",
      "[ time:102.0s total_exs:34296 epochs:3.81 time_left:32.0s ] {'exs': 680, 'lr': 1, 'num_updates': 0, 'examples': 665, 'loss': 4.172, 'mean_loss': 0.006273, 'mean_rank': 0.9985}\n",
      "[ time:104.0s total_exs:34960 epochs:3.88 time_left:30.0s ] {'exs': 664, 'lr': 1, 'num_updates': 0, 'examples': 643, 'loss': 35.2, 'mean_loss': 0.05474, 'mean_rank': 1.012}\n",
      "[ time:106.0s total_exs:35597 epochs:3.96 time_left:29.0s ] {'exs': 637, 'lr': 1, 'num_updates': 0, 'examples': 608, 'loss': 19.99, 'mean_loss': 0.03288, 'mean_rank': 1.01}\n",
      "[ time:108.0s total_exs:36266 epochs:4.03 time_left:27.0s ] {'exs': 669, 'lr': 1, 'num_updates': 0, 'examples': 649, 'loss': 8.016, 'mean_loss': 0.01235, 'mean_rank': 1.003}\n",
      "[ time:110.0s total_exs:36932 epochs:4.1 time_left:25.0s ] {'exs': 666, 'lr': 1, 'num_updates': 0, 'examples': 646, 'loss': 8.868, 'mean_loss': 0.01373, 'mean_rank': 1.002}\n",
      "[ time:112.0s total_exs:37595 epochs:4.18 time_left:23.0s ] {'exs': 663, 'lr': 1, 'num_updates': 0, 'examples': 653, 'loss': 8.899, 'mean_loss': 0.01363, 'mean_rank': 1.0}\n",
      "[ time:114.0s total_exs:38215 epochs:4.25 time_left:21.0s ] {'exs': 620, 'lr': 1, 'num_updates': 0, 'examples': 604, 'loss': 3.784, 'mean_loss': 0.006265, 'mean_rank': 1.0}\n",
      "[ time:116.0s total_exs:38868 epochs:4.32 time_left:19.0s ] {'exs': 653, 'lr': 1, 'num_updates': 0, 'examples': 641, 'loss': 8.757, 'mean_loss': 0.01366, 'mean_rank': 1.002}\n",
      "[ time:118.0s total_exs:39534 epochs:4.39 time_left:17.0s ] {'exs': 666, 'lr': 1, 'num_updates': 0, 'examples': 648, 'loss': 15.37, 'mean_loss': 0.02371, 'mean_rank': 1.003}\n",
      "[ time:120.0s total_exs:40189 epochs:4.47 time_left:15.0s ] {'exs': 655, 'lr': 1, 'num_updates': 0, 'examples': 636, 'loss': 12.92, 'mean_loss': 0.02032, 'mean_rank': 1.005}\n",
      "[ time:122.0s total_exs:40804 epochs:4.53 time_left:13.0s ] {'exs': 615, 'lr': 1, 'num_updates': 0, 'examples': 599, 'loss': 6.343, 'mean_loss': 0.01059, 'mean_rank': 0.9983}\n",
      "[ time:124.0s total_exs:41485 epochs:4.61 time_left:11.0s ] {'exs': 681, 'lr': 1, 'num_updates': 0, 'examples': 659, 'loss': 8.931, 'mean_loss': 0.01355, 'mean_rank': 1.008}\n",
      "[ time:126.0s total_exs:42123 epochs:4.68 time_left:9.0s ] {'exs': 638, 'lr': 1, 'num_updates': 0, 'examples': 617, 'loss': 0.6142, 'mean_loss': 0.0009954, 'mean_rank': 0.9919}\n",
      "[ time:128.0s total_exs:42758 epochs:4.75 time_left:7.0s ] {'exs': 635, 'lr': 1, 'num_updates': 0, 'examples': 615, 'loss': 1.22, 'mean_loss': 0.001984, 'mean_rank': 1.002}\n",
      "[ time:130.0s total_exs:43425 epochs:4.83 time_left:5.0s ] {'exs': 667, 'lr': 1, 'num_updates': 0, 'examples': 651, 'loss': 0.7765, 'mean_loss': 0.001193, 'mean_rank': 1.0}\n",
      "[ time:132.0s total_exs:44095 epochs:4.9 time_left:3.0s ] {'exs': 670, 'lr': 1, 'num_updates': 0, 'examples': 647, 'loss': 5.794, 'mean_loss': 0.008954, 'mean_rank': 1.011}\n",
      "[ time:134.0s total_exs:44759 epochs:4.97 time_left:1.0s ] {'exs': 664, 'lr': 1, 'num_updates': 0, 'examples': 650, 'loss': 7.576, 'mean_loss': 0.01166, 'mean_rank': 0.9954}\n",
      "[ time:134.0s total_exs:45000 epochs:5.0 time_left:0s ] {'exs': 241, 'lr': 1, 'num_updates': 0, 'examples': 232, 'loss': 0.002488, 'mean_loss': 1.073e-05, 'mean_rank': 1.0}\n",
      "[ num_epochs completed:5.0 time elapsed:134.9334671497345s ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ thread 0 initialized ]\n",
      "[ thread 1 initialized ]\n",
      "[ thread 2 initialized ]\n",
      "[ thread 3 initialized ]\n",
      "[ running eval: valid ]\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "valid:{'exs': 1000, 'accuracy': 0.999, 'f1': 0.999, 'hits@1': 0.999, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.99e-10, 'lr': 1, 'num_updates': 0, 'examples': 970, 'loss': 12.11, 'mean_loss': 0.01248, 'mean_rank': 0.9938}\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
      "[ thread 0 initialized ]\n",
      "[ thread 1 initialized ]\n",
      "[ thread 2 initialized ]\n",
      "[ thread 3 initialized ]\n",
      "[ running eval: test ]\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "test:{'exs': 1000, 'accuracy': 0.998, 'f1': 0.998, 'hits@1': 0.998, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.98e-10, 'lr': 1, 'num_updates': 0, 'examples': 967, 'loss': 1.93, 'mean_loss': 0.001996, 'mean_rank': 0.9959}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -mf /tmp/babi_memnn -bs 1 -nt 4 -eps 5 -m memnn --no-cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhtffWNi5WTl"
   },
   "source": [
    "# Displaying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mxClKVXa0Xvp",
    "outputId": "886f0f74-73ef-4fa1-cdbb-e77d29d44ca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: memnn ]\n",
      "[  model_file: /tmp/babi_memnn ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ MemNN Arguments: ] \n",
      "[  embedding_size: 128 ]\n",
      "[  hops: 3 ]\n",
      "[  memsize: 32 ]\n",
      "[  position_encoding: False ]\n",
      "[  time_features: True ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: True ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: True ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ TorchRankerAgent: ] \n",
      "[  candidates: inline ]\n",
      "[  cap_num_predictions: 100 ]\n",
      "[  encode_candidate_vecs: False ]\n",
      "[  eval_candidates: inline ]\n",
      "[  fixed_candidate_vecs: reuse ]\n",
      "[  fixed_candidates_path: None ]\n",
      "[  ignore_bad_candidates: False ]\n",
      "[  train_predict: False ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Dictionary: loading dictionary from /tmp/babi_memnn.dict\n",
      "[ num words =  26 ]\n",
      "Loading existing model parameters from /tmp/babi_memnn\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
      "  ''.format(mode)\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:438: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  matches = ((cand_vecs == label_vec).sum(1) == cand_vecs.size(1)).nonzero()\n",
      "[memory_vecs]:\n",
      "  tensor([14, 23,  7,  6, 21,  5])\n",
      "  tensor([14, 11,  7,  6, 16,  5])\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: bathroom\n",
      "   [text_candidates: bathroom|office|garden|bedroom|kitchen|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 23,  7,  6, 21,  5])\n",
      "  tensor([14, 11,  7,  6, 16,  5])\n",
      "  tensor([ 9, 10, 14,  8])\n",
      "  tensor([16])\n",
      "  tensor([12, 11,  7,  6, 20,  5])\n",
      "  ...and 1 more\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: bathroom\n",
      "   [text_candidates: bathroom|office|garden|kitchen|bedroom|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 23,  7,  6, 21,  5])\n",
      "  tensor([14, 11,  7,  6, 16,  5])\n",
      "  tensor([ 9, 10, 14,  8])\n",
      "  tensor([16])\n",
      "  tensor([12, 11,  7,  6, 20,  5])\n",
      "  ...and 5 more\n",
      "[eval_labels_choice]: bathroom\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: bathroom\n",
      "   [text_candidates: bathroom|office|garden|kitchen|bedroom|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 23,  7,  6, 21,  5])\n",
      "  tensor([14, 11,  7,  6, 16,  5])\n",
      "  tensor([ 9, 10, 14,  8])\n",
      "  tensor([16])\n",
      "  tensor([12, 11,  7,  6, 20,  5])\n",
      "  ...and 9 more\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: office\n",
      "   [text_candidates: office|bathroom|kitchen|garden|hallway|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 23,  7,  6, 21,  5])\n",
      "  tensor([14, 11,  7,  6, 16,  5])\n",
      "  tensor([ 9, 10, 14,  8])\n",
      "  tensor([16])\n",
      "  tensor([12, 11,  7,  6, 20,  5])\n",
      "  ...and 13 more\n",
      "[eval_labels_choice]: hallway\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "bathroom\n",
      "Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "office\n",
      "John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: hallway\n",
      "   [text_candidates: hallway|bathroom|office|garden|bedroom|...and 1 more]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 11, 24,  7,  6, 16,  5])\n",
      "  tensor([12, 25,  7,  6, 18,  5])\n",
      "[eval_labels_choice]: garden\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: garden\n",
      "   [text_candidates: garden|bathroom|kitchen|hallway|bedroom|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 11, 24,  7,  6, 16,  5])\n",
      "  tensor([12, 25,  7,  6, 18,  5])\n",
      "  tensor([ 9, 10, 12,  8])\n",
      "  tensor([18])\n",
      "  tensor([12, 11, 24,  7,  6, 17,  5])\n",
      "  ...and 1 more\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: office\n",
      "   [text_candidates: office|bathroom|garden|kitchen|hallway|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 11, 24,  7,  6, 16,  5])\n",
      "  tensor([12, 25,  7,  6, 18,  5])\n",
      "  tensor([ 9, 10, 12,  8])\n",
      "  tensor([18])\n",
      "  tensor([12, 11, 24,  7,  6, 17,  5])\n",
      "  ...and 5 more\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: office\n",
      "   [text_candidates: office|bathroom|garden|kitchen|bedroom|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 11, 24,  7,  6, 16,  5])\n",
      "  tensor([12, 25,  7,  6, 18,  5])\n",
      "  tensor([ 9, 10, 12,  8])\n",
      "  tensor([18])\n",
      "  tensor([12, 11, 24,  7,  6, 17,  5])\n",
      "  ...and 9 more\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: office\n",
      "   [text_candidates: office|bathroom|kitchen|hallway|garden|...and 1 more]\n",
      "~~\n",
      "[memory_vecs]:\n",
      "  tensor([14, 11, 24,  7,  6, 16,  5])\n",
      "  tensor([12, 25,  7,  6, 18,  5])\n",
      "  tensor([ 9, 10, 12,  8])\n",
      "  tensor([18])\n",
      "  tensor([12, 11, 24,  7,  6, 17,  5])\n",
      "  ...and 13 more\n",
      "[eval_labels_choice]: office\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "garden\n",
      "Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "office\n",
      "Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "office\n",
      "Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: hallway|kitchen|office|garden|bathroom|...and 1 more]\n",
      "   [MemNN]: office\n",
      "   [text_candidates: office|bathroom|kitchen|garden|hallway|...and 1 more]\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -m memnn -t babi:task10k:1 -mf /tmp/babi_memnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxVisshI4yBc"
   },
   "source": [
    "# Interacting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iUnAlYMDrmi"
   },
   "source": [
    "### In this part of Question/Answering on the bAbI task we notice that if we give to the model only the context and the question, it still coninue to give only one answer for any question. It looks like that it memorize the first answer and uses it for any question. To avoid that think we use the concept of 'label candate' and in that case it gives the right answer among the label candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DfQ3B-nY0Xti",
    "outputId": "f8dd4879-f4db-4659-c69e-fc167ed1bfad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: None ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: /tmp/babi_memnn ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ MemNN Arguments: ] \n",
      "[  embedding_size: 128 ]\n",
      "[  hops: 3 ]\n",
      "[  memsize: 32 ]\n",
      "[  position_encoding: False ]\n",
      "[  time_features: True ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: True ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: True ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ TorchRankerAgent: ] \n",
      "[  candidates: inline ]\n",
      "[  cap_num_predictions: 100 ]\n",
      "[  encode_candidate_vecs: False ]\n",
      "[  eval_candidates: vocab ]\n",
      "[  fixed_candidate_vecs: reuse ]\n",
      "[  fixed_candidates_path: None ]\n",
      "[  ignore_bad_candidates: False ]\n",
      "[  train_predict: False ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ warning: overriding opt['eval_candidates'] to vocab (previously: inline )]\n",
      "Dictionary: loading dictionary from /tmp/babi_memnn.dict\n",
      "[ num words =  26 ]\n",
      "Loading existing model parameters from /tmp/babi_memnn\n",
      "[ Loaded fixed candidate set (n = 25) from vocabulary ]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "[ Loaded fixed candidate set (n = 25) from vocabulary ]\n",
      "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 4 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: memnn ]\n",
      "[  model_file: /tmp/babi_memnn ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ MemNN Arguments: ] \n",
      "[  embedding_size: 128 ]\n",
      "[  hops: 3 ]\n",
      "[  memsize: 32 ]\n",
      "[  position_encoding: False ]\n",
      "[  time_features: True ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: True ]\n",
      "[  betas: [0.9, 0.999] ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: True ]\n",
      "[  nus: [0.7] ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: True ]\n",
      "[  split_lines: True ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ TorchRankerAgent: ] \n",
      "[  candidates: inline ]\n",
      "[  cap_num_predictions: 100 ]\n",
      "[  encode_candidate_vecs: False ]\n",
      "[  eval_candidates: vocab ]\n",
      "[  fixed_candidate_vecs: reuse ]\n",
      "[  fixed_candidates_path: None ]\n",
      "[  ignore_bad_candidates: False ]\n",
      "[  train_predict: False ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/babi_memnn.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Enter Your Message: Mary moved to the garden.\\n Where is Mary?\n",
      "/root/ParlAI/parlai/core/torch_ranker_agent.py:425: UserWarning: [ Executing eval mode with tokens from vocabulary as candidates. ]\n",
      "  ''.format(mode)\n",
      "[MemNN]: garden\n",
      "Enter Your Message: John went to the office.\\n Where is John?\n",
      "[MemNN]: garden\n",
      "Enter Your Message: John went back to the hallway.\\n Where is John?\n",
      "[MemNN]: garden\n",
      "Enter Your Message: John went to the office.\\n Where is John? label_candidates: hallway|kitchen|office|garden|bathroom\n",
      "[MemNN]: office\n",
      "Enter Your Message: John went to the hallway.\\n Where is John? label_candidates: hallway|kitchen|office|garden|bathroom\n",
      "[MemNN]: hallway\n",
      "Enter Your Message: "
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/interactive.py -mf /tmp/babi_memnn -ecands vocab"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy_of_Notebook_2_french_chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
